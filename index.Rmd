---
title: "Activity Prediction"
author: "K.M"
date: "`r Sys.Date()`" 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this project, the random forest method is used to predict the "classe" variable based on other features. The model is built using the "caret" package.

## Feature selection
Since the dataset contains a huge number of predictors, we should first reduce the number of features to be included in the model. The data measures are related to 4 parts (belt, arm, glove and dumbbell). For each there are there exist several variables (roll, pitch, yaw, accelerometer, gyroscope and magnetometer.

The first step is to keep only the variables with a small number of missing values in the data set. The second choice is to remove skewness and kurtosis variables since they are related to the shape of the distribution and their effect may be less important than the other perdictors.
Moreover, we split data into two datasets : training set (70%) and testing set (30%). The testing set will be used to estimate the out of sample error.


```{r, cache=TRUE}
train <- read.csv("pml-training.csv", header = T, na.strings=c("","NA","#DIV/0!"))
propNA <- function(x){
  p <- mean(is.na(x))
  p
}
res <- apply(X = train, MARGIN = 2, FUN = propNA)

newTrain <- train[,res < 0.05]
idx1 <- grep(pattern = "kurtosis", x = names(newTrain))
idx2 <- grep(pattern = "skew", x = names(newTrain))
toRemove <- c(1:8,idx1, idx2) 
finalTrain <- newTrain[complete.cases(newTrain[,-toRemove]),-toRemove]

library(caret)
library(randomForest)
set.seed(1234)
inTrain <- createDataPartition(finalTrain$classe,p=0.7, list = FALSE)
training <- finalTrain[inTrain,]
testing <- finalTrain[-inTrain,]
```


## Building the model

The selected variable are used to build a random forest model :

```{r}

suppressMessages(library(caret))
suppressMessages(library(randomForest))

mod <- randomForest(classe~., data=training)

```

## Cross Validation

The out of sample error is estimated using the test set whic represents almost 30% of the initial data set.
```{r}
pred <- predict(mod, newdata = testing)
predRes <- confusionMatrix(testing$classe, pred)
```

The model gives an accuracy of 99.63%. The detailed results are reported in the following table:
```{r echo=FALSE}
suppressMessages(library(knitr))
kable(predRes$byClass)
```